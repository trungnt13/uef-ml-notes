### Presented papers

* [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
* [End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results](https://arxiv.org/pdf/1412.1602.pdf)
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

### Recommended Reading

* "Written Memories: Understanding, Deriving and Extending the LSTM": from vanilla RNN to complex structure for memorizing in LSTM [[link]](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#fn6)
* "Attention and Augmented Recurrent Neural Networks": awesome read about RNN with great visualization and intuitive explanation [[link]](https://distill.pub/2016/augmented-rnns/)

### Source code

* Implementation of "Attention is all you need" from Google [[Code]](https://github.com/tensorflow/tensor2tensor)
* Another of "Attention is all you need", well documented and very active in issues [[code]](https://github.com/Kyubyong/transformer)